{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv('train.csv', parse_dates=[\"date\"])\n",
    "df_trans = pd.read_csv('transactions.csv', parse_dates=[\"date\"])\n",
    "df_stores = pd.read_csv('stores.csv')\n",
    "df_oil = pd.read_csv('oil.csv', parse_dates=[\"date\"]).rename(columns={\"dcoilwtico\": \"oil\"})\n",
    "df_holi = pd.read_csv('holidays_events.csv', parse_dates=[\"date\"])\n",
    "df_sub = pd.read_csv('sample_submission.csv')\n",
    "df_test = pd.read_csv('test.csv', parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_train\n",
    "\n",
    "### Key takeaways\n",
    "1. Top 5 product families: Grocery I, Beverages, Produce, Cleaning, Dairy.\n",
    "2. Promotions and sales have a positive linear relationships.\n",
    "3. Promotions are most common on Wednesdays, Fridays, and holidays\n",
    "4. Promotions (total sum) started mid 2015 and have steadily increased since then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store count = 54\n",
    "df_train.store_nbr.nunique()\n",
    "\n",
    "# Same stores in training and test sets\n",
    "df_train.store_nbr.unique() == df_test.store_nbr.unique()\n",
    "\n",
    "# Family of products count = 33\n",
    "df_train.family.nunique()\n",
    "\n",
    "# Same family of products in training and test sets\n",
    "df_train.family.unique() == df_test.family.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is every product family reported for every date and store_nbr?\n",
    "# Store numbers - Every one (54 total) reported for every date in training set\n",
    "df_store_nbr = df_train.groupby(['date',])['store_nbr'].nunique().reset_index()\n",
    "df_store_nbr[df_store_nbr['store_nbr'] != 54]\n",
    "\n",
    "# Product families - Every one (33 total) reported for every date and store number\n",
    "df_family = df_train.groupby(['date', 'store_nbr'])['family'].nunique().reset_index()\n",
    "df_family[df_family['family'] != 33]\n",
    "\n",
    "# How about in the test set?\n",
    "# Store numbers - Every one (54 total) reported for every date in training set\n",
    "df_store_nbr2 = df_test.groupby(['date',])['store_nbr'].nunique().reset_index()\n",
    "df_store_nbr2[df_store_nbr2['store_nbr'] != 54]\n",
    "\n",
    "# Product families - Every one (33 total) reported for every date and store number\n",
    "df_family2 = df_test.groupby(['date', 'store_nbr'])['family'].nunique().reset_index()\n",
    "df_family2[df_family2['family'] != 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular\n",
    "# Most popular product families by sales - Top 5: Grocery I, Beverages, Produce, Cleaning, Dairy\n",
    "df_train.groupby(['family'])['sales'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Most popular product families by sales by store - Top 5: Beverages, Cleaning, Grocery I, Produce, Dairy\n",
    "# Little variation by store. Only 13 top 5 entries not in list above.\n",
    "df_salesFamilyStore = df_train.groupby(['store_nbr', 'family'])['sales'].sum().reset_index()\n",
    "\n",
    "PopFamily = []\n",
    "for i in df_salesFamilyStore.store_nbr.unique():\n",
    "    PopFamily.append(df_salesFamilyStore[df_salesFamilyStore['store_nbr'] == i].nlargest(5, 'sales'))\n",
    "dfPopFamily = pd.concat(PopFamily)\n",
    "\n",
    "dfPopFamily.groupby(['family'])['family'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular promotion items - Top 5: Grocery I, Produce, Beverages, Dairy, Cleaning\n",
    "df_train.groupby(['family'])['onpromotion'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Most popular product families by promotion by store - Top 6 (only 6): Beverages, Grocery I, Produce, Cleaning, Dairy, Deli\n",
    "df_promoFamilyStore = df_train.groupby(['store_nbr', 'family'])['onpromotion'].sum().reset_index()\n",
    "\n",
    "promoPopFamily = []\n",
    "for i in df_promoFamilyStore.store_nbr.unique():\n",
    "    promoPopFamily.append(df_promoFamilyStore[df_promoFamilyStore['store_nbr'] == i].nlargest(5, 'onpromotion'))\n",
    "dfPromoPopFamily = pd.concat(promoPopFamily)\n",
    "\n",
    "dfPromoPopFamily.groupby(['family'])['family'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a relationship between promotions and sales?\n",
    "# Generally, there is a positive linear relationship between promotions and sales.\n",
    "# However, product families with few promotions and sales deviate from this relationship.\n",
    "dfPlotPromoSales = (df_train.groupby(['family'])[['sales', 'onpromotion']]\n",
    "                    .sum()\n",
    "                    .sort_values(by='onpromotion'))\n",
    "\n",
    "x = dfPlotPromoSales['onpromotion']\n",
    "y = dfPlotPromoSales['sales']\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y)\n",
    "\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x,p(x),\"r--\")\n",
    "\n",
    "plt.xlabel('sales - sum')\n",
    "plt.ylabel('onpromotion - sum')\n",
    "\n",
    "subax = fig.add_axes([0.65, 0.2, 0.2, 0.2])\n",
    "x_labelsize = subax.get_xticklabels()[0].get_size()\n",
    "y_labelsize = subax.get_yticklabels()[0].get_size()\n",
    "subax.xaxis.set_tick_params(labelsize=6)\n",
    "subax.yaxis.set_tick_params(labelsize=6)\n",
    "subax.scatter(x[:15], y[:15])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there certain dates popular for promotions?\n",
    "# Promotions have increased in popularity over time, especially since mid 2015.\n",
    "dfPlotPromoDates = df_train.groupby(['date'])['onpromotion'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoDates['date'], dfPlotPromoDates['onpromotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if I disregard year?\n",
    "# Looks like promotions spike in the middle and at the end of the year.\n",
    "# But this might not be true every year since there are many more promotions\n",
    "# in the later years of the training set.\n",
    "df_train2 = df_train.copy()\n",
    "\n",
    "df_train2['month-day'] = df_train2['date'].apply(lambda x: x.replace(year = 2020))\n",
    "\n",
    "dfPlotPromoMonthDay = df_train2.groupby(['month-day'])['onpromotion'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoMonthDay['month-day'], dfPlotPromoMonthDay['onpromotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at promotions on a yearly basis\n",
    "# 2013 - no promotions\n",
    "# 2014-2018 - looks like intraweek variability\n",
    "# Not too much trend in time of year.\n",
    "dfPlotPromoYear = dfPlotPromoDates[(dfPlotPromoDates['date'] >= '2017-01-01')\n",
    "                                   & (dfPlotPromoDates['date'] < '2018-01-01')]\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoYear['date'], dfPlotPromoYear['onpromotion'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at promotions by day of week\n",
    "# Wednesdays and Fridays have nearly double the promotions of other days\n",
    "df_train2['day'] = df_train2['date'].apply(lambda x: x.isoweekday())\n",
    "dfPlotPromoDay = df_train2.groupby(['day'])['onpromotion'].sum().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoDay['day'], dfPlotPromoDay['onpromotion'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about promotions on public sector paydays (15th and last day of month)\n",
    "# No trend here. Promotions drop on the 31st since some months don't have 31 days...\n",
    "# ...I'm looking at the sum of promotions over all dates in the training set\n",
    "df_train2['day-date'] = df_train2['date'].dt.day\n",
    "\n",
    "dfPlotPromoDayDate = df_train2.groupby(['day-date'])['onpromotion'].sum().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoDayDate['day-date'], dfPlotPromoDayDate['onpromotion'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about promotions on national holidays\n",
    "# ~40% more promotions on national and all holidays than other days\n",
    "\n",
    "# Just national holidays\n",
    "df_train3 = pd.merge(df_train2, df_holi[df_holi['locale'] == 'National'][['date', 'type']], how='left', on='date')\n",
    "# All holidays\n",
    "# df_train3 = pd.merge(df_train2, df_holi[['date', 'type']], how='left', on='date')\n",
    "\n",
    "df_train3['holiday'] = 1\n",
    "df_train3.loc[df_train3['type'].isna(), 'holiday'] = 0\n",
    "\n",
    "dfPlotPromoHoliday = df_train3.groupby(['holiday']).agg({'onpromotion': 'sum', 'date':'nunique'}).reset_index()\n",
    "\n",
    "dfPlotPromoHoliday['norm'] = dfPlotPromoHoliday['onpromotion'] / dfPlotPromoHoliday['date']\n",
    "\n",
    "dfPlotPromoHoliday.loc[1, 'norm'] / dfPlotPromoHoliday.loc[0, 'norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_trans\n",
    "\n",
    "### Key takeaways\n",
    "1. Eight of the 54 stores seem to have opened after the start date of the training set (2013-01-01).\n",
    "2. The capital city (Quito) and state (Pinchincha) are home to 33% and 35% of total stores, respectively.\n",
    "3. 75% (12 of 16) of states only have stores in one city.\n",
    "4. 43% (7 of 16) of states only have one store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First date with transactions by store\n",
    "# Only 8 of 54 stores haven't logged transactions since the beginning of the training set\n",
    "# Those 8 stores must have been opened after 2013-01-01\n",
    "dfFirstTrans = df_trans.groupby('store_nbr')['date'].min().reset_index()\n",
    "\n",
    "dfFirstTrans.groupby('date')['store_nbr'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores with most transactions\n",
    "dfTransMax = df_trans.groupby('store_nbr')['transactions'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Store with most transactions has 26x more than store with least transactions\n",
    "dfTransMax.loc[0, 'transactions'] / dfTransMax.loc[53, 'transactions']\n",
    "\n",
    "dfTransMax.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions by locale\n",
    "dfTransGeo = pd.merge(dfTransMax, df_stores, how='left', on='store_nbr')\n",
    "\n",
    "# Group by city\n",
    "# Stores in Cayambe (1) and Quito (18) average the most transactions per store\n",
    "dfTransCity = dfTransGeo.groupby(['city']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()\n",
    "dfTransCity['norm'] = dfTransCity['transactions'] / dfTransCity['store_nbr']\n",
    "dfTransCity.sort_values(by='norm', ascending=False)\n",
    "\n",
    "# Group by state\n",
    "# Stores in the capital state (Pichincha) average ~25% more transactions than the second most state\n",
    "dfTransState = dfTransGeo.groupby(['state']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()\n",
    "dfTransState['norm'] = dfTransState['transactions'] / dfTransState['store_nbr']\n",
    "dfTransState.sort_values(by='norm', ascending=False)\n",
    "\n",
    "# 1/3 of stores are in Quito\n",
    "dfTransCity[dfTransCity['city'] == 'Quito']['store_nbr'] / dfTransCity.store_nbr.sum()\n",
    "\n",
    "# 35% are in the state of the capital city, Quito (Pichincha)\n",
    "dfTransState[dfTransState['state'] == 'Pichincha']['store_nbr'] / dfTransState.store_nbr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by type\n",
    "# There are only 5 store types. Type A averages more than double the number of transactions\n",
    "# per store than second place Type D\n",
    "dfTransType = dfTransGeo.groupby(['type']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()\n",
    "dfTransType['norm'] = dfTransType['transactions'] / dfTransType['store_nbr']\n",
    "dfTransType.sort_values(by='norm', ascending=False)\n",
    "\n",
    "# Group by cluster\n",
    "# There are 17 clusters with 1-7 stores each. The average number of transactions\n",
    "# per store per cluster varies wildly (2.8 +/- 1.6).\n",
    "dfTransClus = dfTransGeo.groupby(['cluster']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()\n",
    "dfTransClus['norm'] = dfTransClus['transactions'] / dfTransClus['store_nbr']\n",
    "dfTransClus.sort_values(by='norm', ascending=False)\n",
    "\n",
    "dfTransClus.describe()\n",
    "\n",
    "# States and cities can have stores of different types\n",
    "dfTransGeo.groupby(['state', 'city', 'type']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% of states only have stores in one city\n",
    "dfCityState = dfTransGeo.groupby(['state',]).agg({'store_nbr': 'nunique', 'city': 'nunique'}).reset_index()\n",
    "dfCityState[dfCityState['city'] == 1]['state'].count() / dfCityState.state.count()\n",
    "\n",
    "# ~43% of states only have one store\n",
    "dfCityState[dfCityState['store_nbr'] == 1]['state'].count() / dfCityState.state.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_oil\n",
    "\n",
    "### Key takeaways\n",
    "1. Oil prices moved from an average of 99 (dollars? per unit) to 47 (dollars? per unit) suddenly at the end of 2014.\n",
    "2. Prices were relatively stable otherwise with the exception of 3 or 4 swings.\n",
    "3. 43 dates are missing a daily oil price.\n",
    "4. Weekends are not included in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot oil prices\n",
    "x = df_oil['date']\n",
    "y = df_oil['oil']\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Average 'high' price\n",
    "x1 = x[:450]\n",
    "y1 = [y[:450].mean()]*450\n",
    "plt.plot(x1, y1,\"r--\")\n",
    "plt.text(16350, 98, round(y1[0], 2), color='red')\n",
    "\n",
    "# Average 'low' price\n",
    "x2 = x[500:]\n",
    "y2 = [y[500:].mean()]*718\n",
    "plt.plot(x2, y2,\"r--\")\n",
    "plt.text(16200, 46.5, round(y2[0], 2), color='red')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Daily Oil Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for dates missing a daily price\n",
    "# 43 dates missing a daily price\n",
    "# Weekends are not included in dataset\n",
    "df_oil[df_oil['oil'].isna()].date.count()\n",
    "\n",
    "# Weekdays 5 and 6 (Saturday and Sunday) are missing from the dataset\n",
    "df_oil.date.dt.weekday.reset_index().groupby(['date'])['date'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_holi\n",
    "\n",
    "### Key definitions\n",
    "1.  A holiday on a row with column \"transferred\" equal to True was celebrated on a different date. <br>\n",
    "    This date was treated like a regular day. The corresponding row with column \"type\" equal to Transfer <br>\n",
    "    contains the date the holiday was celebrated.\n",
    "2. \"type\" equal to Bridge is an additional day off added to the holiday.\n",
    "3. \"type\" equal to Work Day is a day worked that is not normally worked (e.g., Saturday) <br>\n",
    "    to make up for  a Bridge day.\n",
    "\n",
    "### Key takeaways:\n",
    "1. Most holidays are at the national or local level.\n",
    "2. Holidays/events by year are not stable. 2012 is light. 2014 has many events related to the World Cup <br>\n",
    "    2016 has many events related to the earthquake.\n",
    "3. Most holidays are in the summer months or at the end of the year.\n",
    "4. Holiday counts dip on Tuesdays and Wednesdays are are relatively stable on other days of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown of holidays by type\n",
    "df_holi.groupby(['type'])['date'].count()\n",
    "\n",
    "# Exclude multiple holidays on same date\n",
    "df_holi.groupby(['type'])['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of transferred holidays: 12\n",
    "len(df_holi[df_holi['transferred'] == True])\n",
    "\n",
    "# Were all holidays marked as \"transferred\" = True celebrated elsewhere?\n",
    "# Yes\n",
    "df_holi[df_holi['transferred'] == True]\n",
    "df_holi[df_holi['type'] == 'Transfer']\n",
    "\n",
    "len(df_holi[df_holi['transferred'] == True]) == len(df_holi[df_holi['type'] == 'Transfer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of \"type\" equal to Bridge and Work Day both equal to 5.\n",
    "# Are all five of each coordinated pairs? \n",
    "# Yes\n",
    "df_holi[df_holi['type'] == 'Bridge']\n",
    "df_holi[df_holi['type'] == 'Work Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown of holidays by locale\n",
    "# Includes 12 duplicates due to transfers\n",
    "df_holi.groupby(['locale'])['date'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the count of holidays/events per year relatively stable?\n",
    "# No. Extra events in 2014 due to the World Cup and in 2016 due to the earthquake.\n",
    "# 2012 is a light year.\n",
    "df_holiYear = df_holi.copy()\n",
    "df_holiYear['year'] = df_holiYear.date.dt.year\n",
    "df_holiYear.groupby(['year']).date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays/events by month\n",
    "# Most holidays in summer months or at end of year\n",
    "df_holiYear['month'] = df_holiYear.date.dt.month\n",
    "df_holiYear.groupby(['month']).date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays/events by day of week\n",
    "# Count dips slightly on Tuesdays and Wednesdays\n",
    "df_holiYear['day'] = df_holiYear['date'].apply(lambda x: x.isoweekday())\n",
    "df_holiYear.groupby(['day']).date.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First training data date = 2013-01-01\n",
    "train_start = df_train.date.min()\n",
    "train_start\n",
    "\n",
    "# Last training data date = 2017-08-15\n",
    "train_end = df_train.date.max()\n",
    "train_end\n",
    "\n",
    "# Check for missing dates (discontinuities) in training data -> Christmas Day\n",
    "missing_dates = pd.date_range(train_start, train_end).difference(df_train.date.unique())\n",
    "missing_dates = missing_dates.strftime(\"%Y-%m-%d\").tolist()\n",
    "print('Dates missing from training dataset: ', missing_dates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
