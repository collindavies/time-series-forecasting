{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Forecast store sales on data from Corporación Favorita, a large Ecuadorian-based grocery retailer. <br>\n",
    "\n",
    "#### Instructions\n",
    "Specifically, build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.<br>\n",
    "\n",
    "**Submission**\n",
    "For each id in the test set, you must predict a value for the sales variable. The file should contain a header and have the following format:<br>\n",
    "id,sales<br>\n",
    "3000888,0.0<br>\n",
    "3000889,0.0<br>\n",
    "3000890,0.0<br>\n",
    "3000891,0.0<br>\n",
    "3000892,0.0<br>\n",
    "etc.\n",
    "\n",
    "\n",
    "### Provided...\n",
    "-> Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices. <br>\n",
    "-> Wages in the public sector are paid every two weeks on the 15th and on the last day of the month. Supermarket sales could be affected by this. <br>\n",
    "-> A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake. <br>\n",
    "-> Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. <br>\n",
    "-> Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge. <br>\n",
    "-> Additional holidays are days added to a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday). <br>\n",
    "\n",
    "**-> Evaluation metric: RMSLE**<br>\n",
    "The RMSLE is calculated as:<br>\n",
    "$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(log(1+y^i)−log(1+y_{i}))^2}$<br>\n",
    "where:<br>\n",
    "\n",
    "$n$\n",
    " is the total number of instances,<br>\n",
    "$y^i$\n",
    " is the predicted value of the target for instance (i),<br>\n",
    "$yi$\n",
    " is the actual value of the target for instance (i), and,<br>\n",
    "$log$\n",
    " is the natural logarithm.<br>\n",
    "\n",
    "### Additional...\n",
    "-> Train dataset period: 2013-01-01 to 2017-08-15 <br>\n",
    "-> Test dataset period: 2017-08-16 to 2017-08-31 <br>\n",
    "-> Christmas day is missing from the train dataset for a few years. The stores are likely closed on Christmas. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv('train.csv', parse_dates=[\"date\"])\n",
    "df_trans = pd.read_csv('transactions.csv', parse_dates=[\"date\"])\n",
    "df_stores = pd.read_csv('stores.csv')\n",
    "df_oil = pd.read_csv('oil.csv', parse_dates=[\"date\"]).rename(columns={\"dcoilwtico\": \"oil\"})\n",
    "df_holi = pd.read_csv('holidays_events.csv', parse_dates=[\"date\"])\n",
    "df_sub = pd.read_csv('sample_submission.csv')\n",
    "df_test = pd.read_csv('test.csv', parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explore data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_train\n",
    "\n",
    "### Key takeaways\n",
    "1. Top 5 product families: Grocery I, Beverages, Produce, Cleaning, Dairy.\n",
    "2. Promotions and sales have a positive linear relationships.\n",
    "3. Promotions are most common on Wednesdays, Fridays, and holidays\n",
    "4. Promotions (total sum) started mid 2015 and have steadily increased since then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store count = 54\n",
    "df_train.store_nbr.nunique()\n",
    "\n",
    "# Same stores in training and test sets\n",
    "df_train.store_nbr.unique() == df_test.store_nbr.unique()\n",
    "\n",
    "# Family of products count = 33\n",
    "df_train.family.nunique()\n",
    "\n",
    "# Same family of products in training and test sets\n",
    "df_train.family.unique() == df_test.family.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total sales by store\n",
    "dfStoreSales = df_train.groupby([\"store_nbr\"]).sales.sum().reset_index().sort_values(by = 'sales', ascending=False)\n",
    "dfStoreSales.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is every product family reported for every date and store_nbr?\n",
    "# Store numbers - Every one (54 total) reported for every date in training set\n",
    "df_store_nbr = df_train.groupby(['date',])['store_nbr'].nunique().reset_index()\n",
    "df_store_nbr[df_store_nbr['store_nbr'] != 54]\n",
    "\n",
    "# Product families - Every one (33 total) reported for every date and store number\n",
    "df_family = df_train.groupby(['date', 'store_nbr'])['family'].nunique().reset_index()\n",
    "df_family[df_family['family'] != 33]\n",
    "\n",
    "# How about in the test set?\n",
    "# Store numbers - Every one (54 total) reported for every date in training set\n",
    "df_store_nbr2 = df_test.groupby(['date',])['store_nbr'].nunique().reset_index()\n",
    "df_store_nbr2[df_store_nbr2['store_nbr'] != 54]\n",
    "\n",
    "# Product families - Every one (33 total) reported for every date and store number\n",
    "df_family2 = df_test.groupby(['date', 'store_nbr'])['family'].nunique().reset_index()\n",
    "df_family2[df_family2['family'] != 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular\n",
    "# Most popular product families by sales - Top 5: Grocery I, Beverages, Produce, Cleaning, Dairy\n",
    "df_train.groupby(['family'])['sales'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Most popular product families by sales by store - Top 5: Beverages, Cleaning, Grocery I, Produce, Dairy\n",
    "# Little variation by store. Only 13 top 5 entries not in list above.\n",
    "df_salesFamilyStore = df_train.groupby(['store_nbr', 'family'])['sales'].sum().reset_index()\n",
    "\n",
    "PopFamily = []\n",
    "for i in df_salesFamilyStore.store_nbr.unique():\n",
    "    PopFamily.append(df_salesFamilyStore[df_salesFamilyStore['store_nbr'] == i].nlargest(5, 'sales'))\n",
    "dfPopFamily = pd.concat(PopFamily)\n",
    "\n",
    "dfPopFamily.groupby(['family'])['family'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular promotion items - Top 5: Grocery I, Produce, Beverages, Dairy, Cleaning\n",
    "df_train.groupby(['family'])['onpromotion'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Most popular product families by promotion by store - Top 6 (only 6): Beverages, Grocery I, Produce, Cleaning, Dairy, Deli\n",
    "df_promoFamilyStore = df_train.groupby(['store_nbr', 'family'])['onpromotion'].sum().reset_index()\n",
    "\n",
    "promoPopFamily = []\n",
    "for i in df_promoFamilyStore.store_nbr.unique():\n",
    "    promoPopFamily.append(df_promoFamilyStore[df_promoFamilyStore['store_nbr'] == i].nlargest(5, 'onpromotion'))\n",
    "dfPromoPopFamily = pd.concat(promoPopFamily)\n",
    "\n",
    "dfPromoPopFamily.groupby(['family'])['family'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a relationship between promotions and sales?\n",
    "# Generally, there is a positive linear relationship between promotions and sales.\n",
    "# However, product families with few promotions and sales deviate from this relationship.\n",
    "dfPlotPromoSales = (df_train.groupby(['family'])[['sales', 'onpromotion']]\n",
    "                    .sum()\n",
    "                    .sort_values(by='onpromotion'))\n",
    "\n",
    "x = dfPlotPromoSales['onpromotion']\n",
    "y = dfPlotPromoSales['sales']\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y)\n",
    "\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x,p(x),\"r--\")\n",
    "\n",
    "plt.xlabel('sales - sum')\n",
    "plt.ylabel('onpromotion - sum')\n",
    "\n",
    "subax = fig.add_axes([0.65, 0.2, 0.2, 0.2])\n",
    "x_labelsize = subax.get_xticklabels()[0].get_size()\n",
    "y_labelsize = subax.get_yticklabels()[0].get_size()\n",
    "subax.xaxis.set_tick_params(labelsize=6)\n",
    "subax.yaxis.set_tick_params(labelsize=6)\n",
    "subax.scatter(x[:15], y[:15])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Spearman Correlation between Sales and Onpromotion by product family: {:,.4f}\".format(dfPlotPromoSales.corr(\"spearman\").sales.loc[\"onpromotion\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there certain dates popular for promotions?\n",
    "# Promotions have increased in popularity over time, especially since mid 2015.\n",
    "dfPlotPromoDates = df_train.groupby(['date'])['onpromotion'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoDates['date'], dfPlotPromoDates['onpromotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if I disregard year?\n",
    "# Looks like promotions spike in the middle and at the end of the year.\n",
    "# But this might not be true every year since there are many more promotions\n",
    "# in the later years of the training set.\n",
    "df_train2 = df_train.copy()\n",
    "\n",
    "df_train2['month-day'] = df_train2['date'].apply(lambda x: x.replace(year = 2020))\n",
    "\n",
    "dfPlotPromoMonthDay = df_train2.groupby(['month-day'])['onpromotion'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoMonthDay['month-day'], dfPlotPromoMonthDay['onpromotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at promotions on a yearly basis\n",
    "# 2013 - no promotions\n",
    "# 2014-2018 - looks like intraweek variability\n",
    "# Not too much trend in time of year.\n",
    "dfPlotPromoYear = dfPlotPromoDates[(dfPlotPromoDates['date'] >= '2017-01-01')\n",
    "                                   & (dfPlotPromoDates['date'] < '2018-01-01')]\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoYear['date'], dfPlotPromoYear['onpromotion'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at promotions by day of week\n",
    "# Wednesdays and Fridays have nearly double the promotions of other days\n",
    "df_train2['day'] = df_train2['date'].apply(lambda x: x.isoweekday())\n",
    "dfPlotPromoDay = df_train2.groupby(['day'])['onpromotion'].sum().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoDay['day'], dfPlotPromoDay['onpromotion'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about promotions on public sector paydays (15th and last day of month)\n",
    "# No trend here. Promotions drop on the 31st since some months don't have 31 days...\n",
    "# ...I'm looking at the sum of promotions over all dates in the training set\n",
    "df_train2['day-date'] = df_train2['date'].dt.day\n",
    "\n",
    "dfPlotPromoDayDate = df_train2.groupby(['day-date'])['onpromotion'].sum().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(dfPlotPromoDayDate['day-date'], dfPlotPromoDayDate['onpromotion'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about promotions on national holidays\n",
    "# ~40% more promotions on national and all holidays than other days\n",
    "\n",
    "# Just national holidays\n",
    "df_train3 = pd.merge(df_train2, df_holi[df_holi['locale'] == 'National'][['date', 'type']], how='left', on='date')\n",
    "# All holidays\n",
    "# df_train3 = pd.merge(df_train2, df_holi[['date', 'type']], how='left', on='date')\n",
    "\n",
    "df_train3['holiday'] = 1\n",
    "df_train3.loc[df_train3['type'].isna(), 'holiday'] = 0\n",
    "\n",
    "dfPlotPromoHoliday = df_train3.groupby(['holiday']).agg({'onpromotion': 'sum', 'date':'nunique'}).reset_index()\n",
    "\n",
    "dfPlotPromoHoliday['norm'] = dfPlotPromoHoliday['onpromotion'] / dfPlotPromoHoliday['date']\n",
    "\n",
    "dfPlotPromoHoliday.loc[1, 'norm'] / dfPlotPromoHoliday.loc[0, 'norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_trans\n",
    "\n",
    "### Key takeaways\n",
    "1. Eight of the 54 stores seem to have opened after the start date of the training set (2013-01-01).\n",
    "2. The capital city (Quito) and state (Pinchincha) are home to 33% and 35% of total stores, respectively.\n",
    "3. 75% (12 of 16) of states only have stores in one city.\n",
    "4. 43% (7 of 16) of states only have one store.\n",
    "5. Transactions spike at the end of the year.\n",
    "6. Transactions decline from the start of the month to about the 10th, then rise.\n",
    "7. Transactions spike on Saturdays and Sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First date with transactions by store\n",
    "# Only 8 of 54 stores haven't logged transactions since the beginning of the training set\n",
    "# Those 8 stores must have been opened after 2013-01-01\n",
    "dfFirstTrans = df_trans.groupby('store_nbr')['date'].min().reset_index()\n",
    "\n",
    "dfFirstTrans.groupby('date')['store_nbr'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores with most transactions\n",
    "dfTransMax = df_trans.groupby('store_nbr')['transactions'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Store with most transactions has 26x more than store with least transactions\n",
    "dfTransMax.loc[0, 'transactions'] / dfTransMax.loc[53, 'transactions']\n",
    "\n",
    "dfTransMax.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions by locale\n",
    "dfTransGeo = pd.merge(dfTransMax, df_stores, how='left', on='store_nbr')\n",
    "\n",
    "# Group by city\n",
    "# Stores in Cayambe (1) and Quito (18) average the most transactions per store\n",
    "dfTransCity = dfTransGeo.groupby(['city']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()\n",
    "dfTransCity['norm'] = dfTransCity['transactions'] / dfTransCity['store_nbr']\n",
    "dfTransCity.sort_values(by='norm', ascending=False)\n",
    "\n",
    "# Group by state\n",
    "# Stores in the capital state (Pichincha) average ~25% more transactions than the second most state\n",
    "dfTransState = dfTransGeo.groupby(['state']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()\n",
    "dfTransState['norm'] = dfTransState['transactions'] / dfTransState['store_nbr']\n",
    "dfTransState.sort_values(by='norm', ascending=False)\n",
    "\n",
    "# 1/3 of stores are in Quito\n",
    "dfTransCity[dfTransCity['city'] == 'Quito']['store_nbr'] / dfTransCity.store_nbr.sum()\n",
    "\n",
    "# 35% are in the state of the capital city, Quito (Pichincha)\n",
    "dfTransState[dfTransState['state'] == 'Pichincha']['store_nbr'] / dfTransState.store_nbr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by type\n",
    "# There are only 5 store types. Type A averages more than double the number of transactions\n",
    "# per store than second place Type D\n",
    "dfTransType = dfTransGeo.groupby(['type']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()\n",
    "dfTransType['norm'] = dfTransType['transactions'] / dfTransType['store_nbr']\n",
    "dfTransType.sort_values(by='norm', ascending=False)\n",
    "\n",
    "# Group by cluster\n",
    "# There are 17 clusters with 1-7 stores each. The average number of transactions\n",
    "# per store per cluster varies wildly (2.8 +/- 1.6).\n",
    "dfTransClus = dfTransGeo.groupby(['cluster']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()\n",
    "dfTransClus['norm'] = dfTransClus['transactions'] / dfTransClus['store_nbr']\n",
    "dfTransClus.sort_values(by='norm', ascending=False)\n",
    "\n",
    "dfTransClus.describe()\n",
    "\n",
    "# States and cities can have stores of different types\n",
    "dfTransGeo.groupby(['state', 'city', 'type']).agg({'transactions': 'sum', 'store_nbr': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% of states only have stores in one city\n",
    "dfCityState = dfTransGeo.groupby(['state',]).agg({'store_nbr': 'nunique', 'city': 'nunique'}).reset_index()\n",
    "dfCityState[dfCityState['city'] == 1]['state'].count() / dfCityState.state.count()\n",
    "\n",
    "# ~43% of states only have one store\n",
    "dfCityState[dfCityState['store_nbr'] == 1]['state'].count() / dfCityState.state.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions over time\n",
    "# Steady increase with spikes at end of year\n",
    "df_transDate = df_trans.groupby('date').transactions.sum().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(df_transDate['date'], df_transDate['transactions'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions by month\n",
    "df_transDate['year-month'] = df_transDate['date'].apply(lambda x: x.replace(day = 1))\n",
    "\n",
    "# Steady increase with spikes at end of year\n",
    "df_transYearMonth = df_transDate.groupby('year-month').transactions.sum().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(df_transYearMonth['year-month'], df_transYearMonth['transactions'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average transactions by day of month\n",
    "# Transactions decline from the start of the month to about the 10th, then rise.\n",
    "df_transDate = df_trans.groupby('date').transactions.sum().reset_index()\n",
    "df_transDate['day'] = df_transDate.date.dt.day\n",
    "df_transDay = df_transDate.groupby('day').transactions.mean().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(df_transDay['day'], df_transDay['transactions'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions by day of week\n",
    "# Transactions spike on Saturdays and Sundays.\n",
    "df_transDate['dayOfWeek'] = df_transDate.date.apply(lambda x: x.isoweekday())\n",
    "df_transDayOfWeek = df_transDate.groupby('dayOfWeek').transactions.mean().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(df_transDayOfWeek['dayOfWeek'], df_transDayOfWeek['transactions'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_oil\n",
    "\n",
    "### Key takeaways\n",
    "1. Oil prices moved from an average of 99 (dollars? per unit) to 47 (dollars? per unit) suddenly at the end of 2014.\n",
    "2. Prices were relatively stable otherwise with the exception of 3 or 4 swings.\n",
    "3. 43 dates are missing a daily oil price.\n",
    "4. Weekends are not included in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot oil prices\n",
    "x = df_oil['date']\n",
    "y = df_oil['oil']\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Average 'high' price\n",
    "x1 = x[:450]\n",
    "y1 = [y[:450].mean()]*450\n",
    "plt.plot(x1, y1,\"r--\")\n",
    "plt.text(16350, 98, round(y1[0], 2), color='red')\n",
    "\n",
    "# Average 'low' price\n",
    "x2 = x[500:]\n",
    "y2 = [y[500:].mean()]*718\n",
    "plt.plot(x2, y2,\"r--\")\n",
    "plt.text(16200, 46.5, round(y2[0], 2), color='red')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Daily Oil Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for dates missing a daily price\n",
    "# 43 dates missing a daily price\n",
    "# Weekends are not included in dataset\n",
    "df_oil[df_oil['oil'].isna()].date.count()\n",
    "\n",
    "# Weekdays 5 and 6 (Saturday and Sunday) are missing from the dataset\n",
    "df_oil.date.dt.weekday.reset_index().groupby(['date'])['date'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_holi\n",
    "\n",
    "### Key definitions\n",
    "1.  A holiday on a row with column \"transferred\" equal to True was celebrated on a different date. <br>\n",
    "    This date was treated like a regular day. The corresponding row with column \"type\" equal to Transfer <br>\n",
    "    contains the date the holiday was celebrated.\n",
    "2. \"type\" equal to Bridge is an additional day off added to the holiday.\n",
    "3. \"type\" equal to Work Day is a day worked that is not normally worked (e.g., Saturday) <br>\n",
    "    to make up for  a Bridge day.\n",
    "\n",
    "### Key takeaways:\n",
    "1. Most holidays are at the national or local level.\n",
    "2. Holidays/events by year are not stable due to events. 2012 is light. <br>\n",
    "    2014 has many events related to the World Cup. 2016 has many events related to the earthquake.\n",
    "3. Most holidays are in the summer months or at the end of the year.\n",
    "4. Holiday counts dip on Tuesdays and Wednesdays are are relatively stable on other days of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown of holidays by type\n",
    "df_holi.groupby(['type'])['date'].count()\n",
    "\n",
    "# Exclude multiple holidays on same date\n",
    "df_holi.groupby(['type'])['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of transferred holidays: 12\n",
    "len(df_holi[df_holi['transferred'] == True])\n",
    "\n",
    "# Were all holidays marked as \"transferred\" = True celebrated elsewhere?\n",
    "# Yes\n",
    "df_holi[df_holi['transferred'] == True]\n",
    "df_holi[df_holi['type'] == 'Transfer']\n",
    "\n",
    "len(df_holi[df_holi['transferred'] == True]) == len(df_holi[df_holi['type'] == 'Transfer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of \"type\" equal to Bridge and Work Day both equal to 5.\n",
    "# Are all five of each coordinated pairs? \n",
    "# Yes\n",
    "df_holi[df_holi['type'] == 'Bridge']\n",
    "df_holi[df_holi['type'] == 'Work Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakdown of holidays by locale\n",
    "# Includes 12 duplicates due to transfers\n",
    "df_holi.groupby(['locale'])['date'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the count of holidays/events per year relatively stable?\n",
    "# No. Extra events in 2014 due to the World Cup and in 2016 due to the earthquake.\n",
    "# 2012 is a light year.\n",
    "df_holiYear = df_holi.copy()\n",
    "df_holiYear['year'] = df_holiYear.date.dt.year\n",
    "df_holiYear.groupby(['year']).date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays/events by month\n",
    "# Most holidays in summer months or at end of year\n",
    "df_holiYear['month'] = df_holiYear.date.dt.month\n",
    "df_holiYear.groupby(['month']).date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays/events by day of week\n",
    "# Count dips slightly on Tuesdays and Wednesdays\n",
    "df_holiYear['day'] = df_holiYear['date'].apply(lambda x: x.isoweekday())\n",
    "df_holiYear.groupby(['day']).date.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing dates in training set\n",
    "# First training data date = 2013-01-01\n",
    "train_start = df_train.date.min()\n",
    "\n",
    "# Last training data date = 2017-08-15\n",
    "train_end = df_train.date.max()\n",
    "\n",
    "# Check for missing dates (discontinuities) in training data -> Christmas Day\n",
    "missing_dates = pd.date_range(train_start, train_end).difference(df_train.date.unique())\n",
    "missing_dates = missing_dates.strftime(\"%Y-%m-%d\").tolist()\n",
    "print('Dates missing from training dataset: ', missing_dates)\n",
    "\n",
    "# Stores are likely closed on Christmas. Add those dates to datset and fill in\n",
    "# sales and onpromotion with 0s\n",
    "# Reindex training data\n",
    "multi_idx = pd.MultiIndex.from_product(\n",
    "    [pd.date_range(train_start, train_end), df_train.store_nbr.unique(), df_train.family.unique()],\n",
    "    names=[\"date\", \"store_nbr\", \"family\"],\n",
    ")\n",
    "df_train = df_train.set_index([\"date\", \"store_nbr\", \"family\"]).reindex(multi_idx).reset_index()\n",
    "\n",
    "# Fill missing values with 0s\n",
    "df_train[[\"sales\", \"onpromotion\"]] = df_train[[\"sales\", \"onpromotion\"]].fillna(0.)\n",
    "df_train.id = df_train.id.interpolate(method=\"linear\") # interpolate linearly as a filler for the 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for continuity in transactions dataset\n",
    "expected_trans = ((train_end - train_start).days + 1) * df_train.store_nbr.nunique()\n",
    "total_trans = len(df_trans)\n",
    "zero_trans = df_train.groupby(['date', 'store_nbr'])['sales'].sum().eq(0).sum()\n",
    "miss_trans = expected_trans - total_trans - zero_trans\n",
    "\n",
    "print('Expected entries: ', expected_trans)\n",
    "print('Total entries:    ', total_trans)\n",
    "print('Store-days with no entry: ', zero_trans)\n",
    "print('Missing store-day entries: ', miss_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate dates, sales, and transactions\n",
    "# Total sales per date for each store\n",
    "dfStoreSalesDaily = df_train.groupby([\"date\", \"store_nbr\"]).sales.sum().reset_index()\n",
    "\n",
    "# Reindex transaction data\n",
    "df_trans = df_trans.merge(\n",
    "    dfStoreSalesDaily,\n",
    "    on=[\"date\", \"store_nbr\"],\n",
    "    how=\"outer\",\n",
    ").sort_values([\"date\", \"store_nbr\"], ignore_index=True)\n",
    "\n",
    "# Fill missing values with 0s for days with zero sales\n",
    "df_trans.loc[df_trans.sales.eq(0), \"transactions\"] = 0.\n",
    "df_trans = df_trans.drop(columns=[\"sales\"])\n",
    "\n",
    "# Fill remaining missing values using linear interpolation\n",
    "df_trans.transactions = df_trans.groupby(\"store_nbr\", group_keys=False).transactions.apply(\n",
    "    lambda x: x.interpolate(method=\"linear\", limit_direction=\"both\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing dates (weekends) from oil data\n",
    "# Reindex oil data\n",
    "oil_start = df_oil.date.min()\n",
    "oil_end = df_oil.date.max()\n",
    "\n",
    "df_oil = df_oil.merge(\n",
    "    pd.DataFrame({\"date\": pd.date_range(oil_start, oil_end)}),\n",
    "    on=\"date\",\n",
    "    how=\"outer\",\n",
    ").sort_values(\"date\", ignore_index=True)\n",
    "\n",
    "# Fill missing values using the last price since the market is closed.\n",
    "df_oil.oil = df_oil.oil.ffill()\n",
    "# Then backfill since first date is 2013-01-01 and the market is closed.\n",
    "df_oil.oil = df_oil.oil.bfill()\n",
    "\n",
    "# Fill missing values using linear interpolation\n",
    "# df_oil.oil = df_oil.oil.interpolate(method=\"linear\", limit_direction=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline holiday/events data\n",
    "# Remove all extraneous text from description\n",
    "df_holi['description'] = df_holi.apply(\n",
    "    lambda x: x.description.lower().replace(x.locale_name.lower(), \"\"), axis=1,\n",
    ").replace(\n",
    "    r\"[+-]\\d+|\\b(de|del|traslado|recupero|puente|guayaquil|santo domingo|cuenca|-)\\b\", \"\", regex=True,\n",
    ").replace(\n",
    "    r\"\\s+|-\", \" \", regex=True,\n",
    ").replace(\n",
    "    r'.* futbol .*', \"futbol\", regex=True,\n",
    ").replace(\n",
    "    r'terremoto .*', \"terremoto\", regex=True,\n",
    ").str.strip()\n",
    "\n",
    "# Remove transferred holidays\n",
    "df_holi = df_holi[df_holi['transferred'] == False]\n",
    "\n",
    "# Extract work days\n",
    "work_days = df_holi[df_holi.type.eq(\"Work Day\")]\n",
    "work_days = work_days[[\"date\", \"type\"]].rename(\n",
    "    columns={\"type\": \"work_day\"}\n",
    ").reset_index(drop=True)\n",
    "work_days.work_day = work_days.work_day.notna().astype(int)\n",
    "\n",
    "hol_df = df_holi[df_holi['type'] != \"Work Day\"].reset_index(drop=True)\n",
    "\n",
    "# Extract local holidays\n",
    "local_holidays = df_holi[df_holi['locale'] == 'Local']\n",
    "local_holidays = local_holidays[[\"date\", \"locale_name\", \"description\"]].rename(\n",
    "    columns={\"locale_name\": \"city\"}\n",
    ").reset_index(drop=True)\n",
    "\n",
    "local_holidays = pd.get_dummies(local_holidays, columns=[\"description\"], prefix=\"loc\")\n",
    "\n",
    "# Extract regional/state holidays\n",
    "regional_holidays = df_holi[df_holi['locale'] == 'Regional']\n",
    "regional_holidays = regional_holidays[[\"date\", \"locale_name\", \"description\"]].rename(\n",
    "    columns={\"locale_name\": \"state\", \"description\": \"provincializacion\"}\n",
    ").reset_index(drop=True)\n",
    "regional_holidays.provincializacion = regional_holidays.provincializacion.eq(\"provincializacion\").astype(int)\n",
    "\n",
    "# Extract national holidays\n",
    "national_holidays = df_holi[df_holi['locale'] == 'National']\n",
    "national_holidays = national_holidays[[\"date\", \"description\"]].reset_index(drop=True)\n",
    "national_holidays = national_holidays[~national_holidays.duplicated()]\n",
    "national_holidays = pd.get_dummies(national_holidays, columns=[\"description\"], prefix=\"nat\")\n",
    "# Different national holidays may fall on the same day\n",
    "national_holidays = national_holidays.groupby(\"date\").sum().reset_index()\n",
    "# Shorten name for visualization purposes later\n",
    "national_holidays = national_holidays.rename(columns={\"nat_primer grito independencia\": \"nat_primer grito\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing dates in test dataset\n",
    "test_start = df_test.date.min()\n",
    "test_end = df_test.date.max()\n",
    "\n",
    "print('Dates missing from test dataset: ', pd.date_range(test_start, test_end).difference(df_test.date.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compile datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile dfs\n",
    "def compile_dfs(df):\n",
    "    temp_df = df.merge(\n",
    "        df_trans, on=[\"date\", \"store_nbr\"], how=\"left\",\n",
    "    ).merge(\n",
    "        df_oil, on=\"date\", how=\"left\",\n",
    "    ).merge(\n",
    "        df_stores, on=\"store_nbr\", how=\"left\",\n",
    "    ).merge(\n",
    "        work_days, on=\"date\", how=\"left\",\n",
    "    ).merge(\n",
    "        local_holidays, on=[\"date\", 'city'], how=\"left\",\n",
    "    ).merge(\n",
    "        regional_holidays, on=[\"date\", 'state'], how=\"left\",\n",
    "    ).merge(\n",
    "        national_holidays, on=\"date\", how=\"left\",\n",
    "    ).sort_values([\"date\", \"store_nbr\", \"family\"], ignore_index=True)\n",
    "\n",
    "    # Fill columns with 0s to indicate absence of holidays/events\n",
    "    return(temp_df.fillna(0))\n",
    "\n",
    "\n",
    "df_trainNew = compile_dfs(df_train)\n",
    "df_testNew = compile_dfs(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Additional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are holidays significant to sales?\n",
    "# Most are!\n",
    "def AB_Test(dataframe, group, target):\n",
    "    \n",
    "    # Packages\n",
    "    from scipy.stats import shapiro\n",
    "    import scipy.stats as stats\n",
    "    \n",
    "    # Split A/B\n",
    "    groupA = dataframe[dataframe[group] == 1][target]\n",
    "    groupB = dataframe[dataframe[group] == 0][target]\n",
    "    \n",
    "    # Assumption: Normality\n",
    "    ntA = shapiro(groupA)[1] < 0.05\n",
    "    ntB = shapiro(groupB)[1] < 0.05\n",
    "    # H0: Distribution is Normal! - False\n",
    "    # H1: Distribution is not Normal! - True\n",
    "\n",
    "    if (ntA == False) & (ntB == False): # \"H0: Normal Distribution\"\n",
    "        # Parametric Test\n",
    "        # Assumption: Homogeneity of variances\n",
    "        leveneTest = stats.levene(groupA, groupB)[1] < 0.05\n",
    "        # H0: Homogeneity: False\n",
    "        # H1: Heterogeneous: True\n",
    "        \n",
    "        if leveneTest == False:\n",
    "            # Homogeneity\n",
    "            ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n",
    "            # H0: M1 == M2 - False\n",
    "            # H1: M1 != M2 - True\n",
    "        else:\n",
    "            # Heterogeneous\n",
    "            ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]\n",
    "            # H0: M1 == M2 - False\n",
    "            # H1: M1 != M2 - True\n",
    "    else:\n",
    "        # Non-Parametric Test\n",
    "        ttest = stats.mannwhitneyu(groupA, groupB)[1] \n",
    "        # H0: M1 == M2 - False\n",
    "        # H1: M1 != M2 - True\n",
    "        \n",
    "    # Result\n",
    "    temp = pd.DataFrame({\n",
    "        \"AB Hypothesis\":[ttest < 0.05], \n",
    "        \"p-value\":[ttest]\n",
    "    })\n",
    "    temp[\"Test Type\"] = np.where((ntA == False) & (ntB == False), \"Parametric\", \"Non-Parametric\")\n",
    "    temp[\"AB Hypothesis\"] = np.where(temp[\"AB Hypothesis\"] == False, \"Fail to Reject H0\", \"Reject H0\")\n",
    "    temp[\"Comment\"] = np.where(temp[\"AB Hypothesis\"] == \"Fail to Reject H0\", \"A/B groups are similar!\", \"A/B groups are not similar!\")\n",
    "    temp[\"Feature\"] = group\n",
    "    temp[\"GroupA_mean\"] = groupA.mean()\n",
    "    temp[\"GroupB_mean\"] = groupB.mean()\n",
    "    temp[\"GroupA_median\"] = groupA.median()\n",
    "    temp[\"GroupB_median\"] = groupB.median()\n",
    "    \n",
    "    # Columns\n",
    "    if (ntA == False) & (ntB == False):\n",
    "        temp[\"Homogeneity\"] = np.where(leveneTest == False, \"Yes\", \"No\")\n",
    "        temp = temp[[\"Feature\",\"Test Type\", \"Homogeneity\",\"AB Hypothesis\", \"p-value\", \"Comment\", \"GroupA_mean\", \"GroupB_mean\", \"GroupA_median\", \"GroupB_median\"]]\n",
    "    else:\n",
    "        temp = temp[[\"Feature\",\"Test Type\",\"AB Hypothesis\", \"p-value\", \"Comment\", \"GroupA_mean\", \"GroupB_mean\", \"GroupA_median\", \"GroupB_median\"]]\n",
    "    \n",
    "    # Print Hypothesis\n",
    "    # print(\"# A/B Testing Hypothesis\")\n",
    "    # print(\"H0: A == B\")\n",
    "    # print(\"H1: A != B\", \"\\n\")\n",
    "    \n",
    "    return temp\n",
    "    \n",
    "# Apply A/B Testing\n",
    "he_cols = df_trainNew.columns[12:].tolist()\n",
    "ab = []\n",
    "for i in he_cols:\n",
    "    ab.append(AB_Test(dataframe=df_trainNew[df_trainNew.sales.notnull()], group = i, target = \"sales\"))\n",
    "ab = pd.concat(ab)\n",
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ Plots - What is data distribution?\n",
    "import statsmodels.graphics.gofplots as sm \n",
    "\n",
    "for i in he_cols:\n",
    "    print(i)\n",
    "    df_trainNewPlotTmp = df_trainNew[df_trainNew['sales'].notnull()]\n",
    "    df_trainNewPlotTmp1 = df_trainNewPlotTmp[df_trainNewPlotTmp[i] == 1]['sales']\n",
    "    df_trainNewPlotTmp2 = df_trainNewPlotTmp[df_trainNewPlotTmp[i] == 0]['sales']\n",
    "    print(\"length of df1:\", len(df_trainNewPlotTmp1))\n",
    "    print(\"length of df2:\", len(df_trainNewPlotTmp2))\n",
    "\n",
    "    fig1, ax = plt.subplots(1, 2, figsize=(12, 7)) \n",
    "    sns.histplot(df_trainNewPlotTmp1, kde=True, color ='blue',ax=ax[0]) \n",
    "    sm.ProbPlot(df_trainNewPlotTmp1).qqplot(line='s', ax=ax[1])\n",
    "\n",
    "    fig2, ax = plt.subplots(1, 2, figsize=(12, 7)) \n",
    "    sns.histplot(df_trainNewPlotTmp2, kde=True, color ='blue',ax=ax[0]) \n",
    "    sm.ProbPlot(df_trainNewPlotTmp1).qqplot(line='s', ax=ax[1])\n",
    "\n",
    "    plt.show(block=False)\n",
    "    plt.pause(3)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
